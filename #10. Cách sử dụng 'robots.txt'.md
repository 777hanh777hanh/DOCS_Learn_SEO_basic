# 10. Cách sử dụng `robots.txt`

`robots.txt` là một tệp văn bản đặc biệt được đặt trên máy chủ web để **hướng dẫn các robot của công cụ tìm kiếm (như Googlebot của Google) về cách thu thập thông tin từ trang web**.

Tệp này có thể chứa các **chỉ định về việc tránh hoặc cho phép robot tìm kiếm truy cập các phần cụ thể** của trang web.

File `robots.txt` được đặt ở folder `root` (thư mục gốc).

-   User-agent

    -   Xác định robot tìm kiếm cụ thể mà các quy tắc áp dụng.

    ```makefile
    User-agent: *
    ```

    cho tất cả các robot tìm kiếm

    ```makefile
    User-agent: Googlebot
    ```

    áp dụng chỉ cho Googlebot

-   Disallow

    -   Chỉ định các phần của trang web mà robot tìm kiếm không nên truy cập

    ```makefile
    Disallow: /private/
    ```

    Các robot không nên truy cập vào bất kỳ trang nào bắt đầu bằng "/private/

-   Allow

    -   Ngược lại với Disallow, chỉ định các phần của trang web mà robot tìm kiếm được phép truy cập.

    ```makefile
    Allow: /public/
    ```

    ```makefile
    Allow: *
    ```

-   Sitemap
    -   Chỉ định vị trí của sitemap của trang web.
    ```makefile
    Sitemap: https://www.example.com/sitemap.xml
    ```

## Ví dụ

```makefile
User-agent: *
Disallow: /cgi-bin/
Disallow: /dangky.html
Disallow: /dangnhap.html
```

-   Có thể chia nhiều User-agent. Mỗi thành phần Disallow, Allow bên dưới sẽ được đặt cho User-agent bên trên gần nhất.

```makefile
User-agent: googlebot
Disallow: /nogooglebot/

User-agent: google-image
Disallow: /private/
```

googlebot sẽ chỉ không được truy cập vào **nogooglebot** còn google-image sẽ không truy cập được **private** nhưng vẫn truy cập được **nogooglebot**

<br/>
<br/>

<hr/>
<br/>

> `robots.txt` là một cách để quản lý cách các robot tìm kiếm tương tác với trang web, nhưng không phải tất cả các robot tìm kiếm đều tuân theo các quy tắc được xác định trong tệp này. Một số robot có thể bỏ qua `robots.txt` và tiếp tục thu thập thông tin từ trang web.
